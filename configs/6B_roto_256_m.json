{
  "layers": 3,
  "d_model": 512,
  "n_heads": 16,
  "n_vocab": 512,
  "norm": "layernorm",
  "pe": "rotary",
  "pe_rotary_dims": 64,

  "seq": 512,
  "cores_per_replica": 8,
  "per_replica_batch": 1,
  "gradient_accumulation_steps": 16,

  "warmup_steps": 3200,
  "anneal_steps": 3200,
  "lr": 1.2e-4,
  "end_lr": 1.2e-5,
  "weight_decay": 0.1,
  "total_steps":10000,

  "tpu_size": 8,

  "bucket": "neo-models",
  "model_dir": "content/mesh-transformer-jax/data/ft_Data",

  "train_set": "openwebtext.train.index",
  "val_set": {
    "owt": "openwebtext.val.index"
  },

  "eval_harness_tasks": [
    "lambada",
    "piqa",
    "hellaswag",
    "winogrande",
    "mathqa",
    "pubmedqa"
  ],

  "val_batches": 1,
  "val_every": 500,
  "ckpt_every": 5000,
  "keep_every": 10000,

  "name": "GPT3_6B_finetune_rotary",
  "wandb_project": "mesh-transformer-jax",
  "comment": ""
}